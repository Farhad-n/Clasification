{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Breed Identification\n",
    "## Farhad Navid \n",
    "\n",
    "### transfer learning with Aug data set\n",
    "\n",
    "* Load data set\n",
    "* Load weights from ImageNet features for pretrained VGG19 model \"block4_pool\".\n",
    "* Run model\n",
    "* Create train and test\n",
    "* Run SVM model\n",
    "* Record the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "import matplotlib.pyplot as plt \n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import deepdish as dd\n",
    "\n",
    "from array import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation, GlobalMaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D \n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix,accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "K.set_image_dim_ordering( 'tf' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code does represent the Directory structure of the data once the repository was selected.  In this instance the [**AWS**](https://www.AWS.Amazon.com/) (paid service) were utilized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory structure of the data\n",
    "\n",
    "dpath_train = '/home/ubuntu/train'      # Data path to training data set\n",
    "dpath_test  = '/home/ubuntu/test'       # Data path to test data set\n",
    "dpath       = '/home/ubuntu'\n",
    "label_f     = '/home/ubuntu/labels.csv' # Path to the label file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the augmented data file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive already created data set with Augmented Images\n",
    "\n",
    "Aug_train_data = '/home/ubuntu/train_aug_data.hdf5' #\n",
    "f= dd.io.load(Aug_train_data)\n",
    "X_train_aug = f['X']\n",
    "y_train_aug = f['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [04:09<00:00, 41.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# point to location of the Train images\n",
    "images = glob.glob('/home/ubuntu/train/*.jpg')\n",
    "img_data = []\n",
    "# resize and add new axis to images\n",
    "for i in tqdm(images):\n",
    "    img = image.load_img(i, target_size=(224, 224))\n",
    "    img_data.append(image.img_to_array(img)[np.newaxis, :, :, :])\n",
    "# concatenate the images    \n",
    "tr_img_data_np = np.concatenate(img_data, )\n",
    "# save them as \"unsigned Integer\" for the smaller size.\n",
    "tr_img_data_np = tr_img_data_np.astype('uint8')\n",
    "# create the label \n",
    "labels = pd.read_csv(label_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10222, 224, 224, 3) uint8\n"
     ]
    }
   ],
   "source": [
    "print(tr_img_data_np.shape,tr_img_data_np.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    @parameters:\n",
    "        dataset: the feature training dataset in numpy array with shape [num_examples, num_rows, num_cols, num_channels] (since it is an image in numpy array)\n",
    "        dataset_labels: the corresponding training labels of the feature training dataset in the same order, and numpy array with shape [num_examples, <anything>]\n",
    "        augmentation_factor: how many times to perform augmentation.\n",
    "        use_random_rotation: whether to use random rotation. default: true\n",
    "        use_random_shift: whether to use random shift. default: true\n",
    "        use_random_shear: whether to use random shear. default: true\n",
    "        use_random_zoom: whether to use random zoom. default: true\n",
    "        \n",
    "    @returns:\n",
    "        augmented_image: augmented dataset\n",
    "        augmented_image_labels: labels corresponding to augmented dataset in order.\n",
    "        \n",
    "    for the augmentation techniques documentation, go here:\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_rotation\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_shear\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_shift\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_zoom\n",
    "'''\n",
    "def augment_data(dataset, dataset_labels, augementation_factor=1, use_random_rotation=True, use_random_shear=True, use_random_shift=True, use_random_zoom=True):\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    for num in tqdm(range(0, dataset.shape[0])):  # tqdm progress bar\n",
    "        for i in range(0, augementation_factor):\n",
    "            # original image:\n",
    "            augmented_image.append(dataset[num])\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_rotation:\n",
    "                \n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_rotation(dataset[num], 20, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shear:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shear(dataset[num], 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shift:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shift(dataset[num], 0.2, 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_zoom:\n",
    "                # update: zoomrange (second arg) should be tuple of floats\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_zoom(dataset[num], (0.9, 0.9), row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [06:55<00:00, 24.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Augment train data set this will increase the data set size by 5X\n",
    "aug_images, aug_labels = augment_data(tr_img_data_np, labels.breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51110, 224, 224, 3) (51110,) uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check point make sure the size and shape are as expected.\n",
    "print(aug_images.shape,aug_labels.shape,aug_images.dtype)\n",
    "# Make sure the data type is expected.\n",
    "aug_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique Dog Breed Numbers =  120\n"
     ]
    }
   ],
   "source": [
    "# Now let's make sure we have all the unique dog names identified and chk the total number.\n",
    "\n",
    "unique_Dog_Breed = []\n",
    "for i in aug_labels:\n",
    "    if i not in unique_Dog_Breed:\n",
    "        unique_Dog_Breed.append(i)\n",
    "    \n",
    "unique_Dog_Breed.sort()   # Now sort the unique lable alphabatically. Next we need to count howmany unique label\n",
    "\n",
    "count = 0\n",
    "unique_Dog_Breed_Num = []   # integer assiciated with every unique Dog name\n",
    "\n",
    "# with the next loops we are creating integers for each unique labels. in prepration of catagorizing the Y.\n",
    "\n",
    "for i in unique_Dog_Breed:\n",
    "    unique_Dog_Breed_Num.append([i, count])\n",
    "    count += 1\n",
    "\n",
    "for i in range(len(aug_labels)):\n",
    "    for j in unique_Dog_Breed_Num:\n",
    "        if aug_labels[i] == j[0]:\n",
    "            aug_labels[i] = j[1]\n",
    "            break\n",
    "\n",
    "print(\"unique Dog Breed Numbers = \",len(unique_Dog_Breed_Num))\n",
    "y_train = to_categorical(aug_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51110, 120) float32\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape, y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save to hdf5 \n",
    "\n",
    "#os.chdir(dpath)\n",
    "with h5py.File('traindata_aug.hdf5','w') as f:\n",
    "    f.create_dataset('X', data=aug_images)\n",
    "    f.create_dataset('Y', data=y_train)\n",
    "# # Save the X_train and y_train to a HD5 file with Deep Dish. \n",
    "# import deepdish as dd\n",
    "# Aug_train_data = '/home/ubuntu/train_aug_data.hdf5'\n",
    "# dd.io.save(Aug_train_data, {'X': aug_images, 'Y': aug_labels}, compression=('blosc', 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = aug_images\n",
    "y_train_aug = aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51110, 224, 224, 3) uint8 (51110,) <U30\n"
     ]
    }
   ],
   "source": [
    "print(X_train_aug.shape,X_train_aug.dtype,y_train_aug.shape,y_train_aug.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "**Only 10000 images of the 51110 images are used**\n",
    "\n",
    "Due to the size of the input file and the number of features for the **block4_pool** could not run the model completely. A different strategy is required to run the full model and that was beyond the time scope of this project.  \n",
    "following is the example of max number of images we could process without getting the memory error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = X_train_aug[:10000] \n",
    "y_train_s = y_train_aug[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:50<00:00, 34.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# transfer learning\n",
    "# Get the weights from imagenet for the VGG19 model\n",
    "base_model = VGG19(weights='imagenet')\n",
    "# now select the layer to get the features from in this case \"block4_pool\" was selected\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\n",
    "\n",
    "# Create a zero Numpy array with the shape of np.zero((Xs_train.shape[0],(block4_pool_features))) \n",
    "# for this modle the (image#,14,14,512)\n",
    "\n",
    "train_set = np.zeros((X_train_s.shape[0],14,14,512))\n",
    "\n",
    "## This section of code was written to findout the dimmention of the Block4_pool_features.\n",
    "\n",
    "# x = preprocess_input(X_train[1])  # get one sample of X_train\n",
    "# print(x.shape)                    # Chk the shape \n",
    "# x = np.expand_dims(x,axis=0)      # Ad the image num to the list\n",
    "# print(x.shape)                    # Check the shape\n",
    "\n",
    "# block4_pool_features_org = model.predict(x)  # Create one entry to see the shape\n",
    "\n",
    "# this loop will fill the train_set numpy array (each x_train runs through model with extracted VGG features)\n",
    "for i in tqdm(range(X_train_s.shape[0])):\n",
    "#    K.clear_session()\n",
    "    x = preprocess_input(X_train_s[i])\n",
    "    x = np.expand_dims(x,axis=0)\n",
    "    block4_pool_features = model.predict(x)\n",
    "    train_set[i]=block4_pool_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14, 14, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block4_pool_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test data set for SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split training data in to train and test. \n",
    "\n",
    "train_set = train_set.reshape(train_set.shape[0],-1)\n",
    "X= train_set\n",
    "y= y_train_s\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model\n",
    "* **Model Fit**\n",
    "* **Model Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22428.387664556503 seconds\n",
      "12288.37120962143 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the svm \n",
    "\n",
    "t0=time.time()\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "t1=time.time()\n",
    "print(t1-t0,\"seconds\")\n",
    "\n",
    "# predict \n",
    "t0=time.time()\n",
    "pred_Train=clf.predict(X_train) \n",
    "pred_Test=clf.predict(X_test) \n",
    "t1=time.time()\n",
    "print(t1-t0,\"seconds\")\n",
    "# print(pred_Train[:10],pred_Test[:10])\n",
    "# print(clf.predict([train_set[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "* Cohen's Kappa Score\n",
    "* Accuracy score\n",
    "* classification report\n",
    "* F1 Score\n",
    "* confusion matrix (This matrix was not selected since the entire model did not run and result was not as interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997758066476816 Train data set cohen kappa score\n",
      "0.00909573426813326 Test data set cohen kappa score\n",
      "0.9997777777777778 Accuracy Score Train Data set\n",
      "0.018 Accuracy Score Test Data set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        81\n",
      "          1       1.00      1.00      1.00        95\n",
      "         10       1.00      1.00      1.00       113\n",
      "        100       1.00      1.00      1.00        89\n",
      "        101       1.00      1.00      1.00        82\n",
      "        102       1.00      1.00      1.00        96\n",
      "        103       1.00      1.00      1.00        40\n",
      "        104       1.00      1.00      1.00        59\n",
      "        105       1.00      1.00      1.00        83\n",
      "        106       1.00      1.00      1.00        64\n",
      "        107       1.00      1.00      1.00        65\n",
      "        108       1.00      1.00      1.00        67\n",
      "        109       1.00      1.00      1.00        74\n",
      "         11       1.00      1.00      1.00       115\n",
      "        110       1.00      1.00      1.00       108\n",
      "        111       1.00      1.00      1.00        69\n",
      "        112       1.00      1.00      1.00        56\n",
      "        113       1.00      1.00      1.00        84\n",
      "        114       1.00      1.00      1.00        60\n",
      "        115       1.00      1.00      1.00        99\n",
      "        116       1.00      1.00      1.00        75\n",
      "        117       1.00      1.00      1.00        98\n",
      "        118       1.00      1.00      1.00        40\n",
      "        119       1.00      1.00      1.00        78\n",
      "         12       1.00      1.00      1.00        55\n",
      "         13       1.00      1.00      1.00       104\n",
      "         14       1.00      1.00      1.00       100\n",
      "         15       1.00      1.00      1.00        71\n",
      "         16       1.00      1.00      1.00        64\n",
      "         17       1.00      1.00      1.00        92\n",
      "         18       0.97      1.00      0.98        65\n",
      "         19       1.00      1.00      1.00        93\n",
      "          2       1.00      1.00      1.00        59\n",
      "         20       1.00      1.00      1.00        85\n",
      "         21       1.00      1.00      1.00        67\n",
      "         22       1.00      1.00      1.00        48\n",
      "         23       1.00      1.00      1.00        38\n",
      "         24       1.00      1.00      1.00        66\n",
      "         25       1.00      1.00      1.00        73\n",
      "         26       1.00      1.00      1.00        77\n",
      "         27       1.00      1.00      1.00        69\n",
      "         28       1.00      1.00      1.00        94\n",
      "         29       1.00      1.00      1.00        70\n",
      "          3       1.00      1.00      1.00        99\n",
      "         30       1.00      1.00      1.00        96\n",
      "         31       1.00      1.00      1.00        75\n",
      "         32       1.00      1.00      1.00        69\n",
      "         33       1.00      1.00      1.00        89\n",
      "         34       1.00      1.00      1.00        62\n",
      "         35       1.00      1.00      1.00        78\n",
      "         36       1.00      1.00      1.00        82\n",
      "         37       1.00      1.00      1.00        60\n",
      "         38       1.00      1.00      1.00        46\n",
      "         39       1.00      1.00      1.00        88\n",
      "          4       1.00      1.00      1.00        60\n",
      "         40       1.00      1.00      1.00        69\n",
      "         41       1.00      1.00      1.00        58\n",
      "         42       1.00      1.00      1.00        86\n",
      "         43       1.00      1.00      1.00        55\n",
      "         44       1.00      0.97      0.98        58\n",
      "         45       1.00      1.00      1.00        56\n",
      "         46       1.00      1.00      1.00        43\n",
      "         47       1.00      1.00      1.00        57\n",
      "         48       1.00      1.00      1.00        87\n",
      "         49       1.00      1.00      1.00        68\n",
      "          5       1.00      1.00      1.00        77\n",
      "         50       1.00      1.00      1.00       100\n",
      "         51       1.00      1.00      1.00        46\n",
      "         52       1.00      1.00      1.00        75\n",
      "         53       1.00      1.00      1.00        59\n",
      "         54       1.00      1.00      1.00        69\n",
      "         55       1.00      1.00      1.00        72\n",
      "         56       1.00      1.00      1.00        91\n",
      "         57       1.00      1.00      1.00        68\n",
      "         58       1.00      1.00      1.00        55\n",
      "         59       1.00      1.00      1.00        80\n",
      "          6       1.00      1.00      1.00        91\n",
      "         60       1.00      1.00      1.00        91\n",
      "         61       1.00      1.00      1.00        75\n",
      "         62       1.00      1.00      1.00        57\n",
      "         63       1.00      1.00      1.00        77\n",
      "         64       1.00      1.00      1.00        60\n",
      "         65       1.00      1.00      1.00        59\n",
      "         66       1.00      1.00      1.00        59\n",
      "         67       1.00      1.00      1.00        76\n",
      "         68       1.00      1.00      1.00       121\n",
      "         69       1.00      1.00      1.00        79\n",
      "          7       1.00      1.00      1.00       116\n",
      "         70       1.00      1.00      1.00        83\n",
      "         71       1.00      1.00      1.00        88\n",
      "         72       1.00      1.00      1.00        60\n",
      "         73       1.00      1.00      1.00        87\n",
      "         74       1.00      1.00      1.00        66\n",
      "         75       1.00      1.00      1.00        80\n",
      "         76       1.00      1.00      1.00        55\n",
      "         77       1.00      1.00      1.00        57\n",
      "         78       1.00      1.00      1.00        72\n",
      "         79       1.00      1.00      1.00        83\n",
      "          8       1.00      1.00      1.00        77\n",
      "         80       1.00      1.00      1.00        90\n",
      "         81       1.00      1.00      1.00        68\n",
      "         82       1.00      1.00      1.00        83\n",
      "         83       1.00      1.00      1.00        58\n",
      "         84       1.00      1.00      1.00        80\n",
      "         85       1.00      1.00      1.00        63\n",
      "         86       1.00      1.00      1.00        74\n",
      "         87       1.00      1.00      1.00        66\n",
      "         88       1.00      1.00      1.00        93\n",
      "         89       1.00      1.00      1.00        78\n",
      "          9       1.00      1.00      1.00        70\n",
      "         90       1.00      1.00      1.00        85\n",
      "         91       1.00      1.00      1.00        61\n",
      "         92       1.00      1.00      1.00        42\n",
      "         93       1.00      1.00      1.00       108\n",
      "         94       1.00      1.00      1.00       107\n",
      "         95       1.00      1.00      1.00        81\n",
      "         96       1.00      1.00      1.00        67\n",
      "         97       1.00      1.00      1.00       106\n",
      "         98       1.00      1.00      1.00        65\n",
      "         99       1.00      1.00      1.00        73\n",
      "\n",
      "avg / total       1.00      1.00      1.00      9000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00         9\n",
      "          1       0.00      0.00      0.00        10\n",
      "         10       0.00      0.00      0.00        17\n",
      "        100       1.00      0.06      0.12        16\n",
      "        101       0.00      0.00      0.00         8\n",
      "        102       0.00      0.00      0.00        14\n",
      "        104       1.00      0.17      0.29         6\n",
      "        105       0.00      0.00      0.00         7\n",
      "        106       0.00      0.00      0.00        11\n",
      "        107       0.00      0.00      0.00         5\n",
      "        108       0.00      0.00      0.00         8\n",
      "        109       0.00      0.00      0.00        11\n",
      "         11       0.00      0.00      0.00        15\n",
      "        110       0.00      0.00      0.00        12\n",
      "        111       0.00      0.00      0.00         6\n",
      "        112       0.00      0.00      0.00         4\n",
      "        113       0.00      0.00      0.00         6\n",
      "        114       0.00      0.00      0.00        10\n",
      "        115       0.00      0.00      0.00         6\n",
      "        116       0.00      0.00      0.00         5\n",
      "        117       0.00      0.00      0.00        12\n",
      "        118       0.00      0.00      0.00        10\n",
      "        119       0.00      0.00      0.00         7\n",
      "         12       0.00      0.00      0.00         5\n",
      "         13       0.00      0.00      0.00        16\n",
      "         14       0.00      0.00      0.00        15\n",
      "         15       0.00      0.00      0.00         4\n",
      "         16       0.00      0.00      0.00         6\n",
      "         17       0.00      0.00      0.00         8\n",
      "         18       0.00      0.00      0.00        10\n",
      "         19       0.00      0.00      0.00        12\n",
      "          2       1.00      0.17      0.29         6\n",
      "         20       0.00      0.00      0.00         5\n",
      "         21       0.00      0.00      0.00         8\n",
      "         22       0.00      0.00      0.00         2\n",
      "         23       0.00      0.00      0.00         2\n",
      "         24       0.00      0.00      0.00         4\n",
      "         25       0.00      0.00      0.00         7\n",
      "         26       0.00      0.00      0.00         8\n",
      "         27       0.00      0.00      0.00        16\n",
      "         28       0.00      0.00      0.00         6\n",
      "         29       0.00      0.00      0.00         5\n",
      "          3       0.00      0.00      0.00        11\n",
      "         30       0.00      0.00      0.00        14\n",
      "         31       1.00      0.20      0.33         5\n",
      "         32       0.00      0.00      0.00         6\n",
      "         33       0.00      0.00      0.00        11\n",
      "         34       0.00      0.00      0.00         8\n",
      "         35       0.00      0.00      0.00        12\n",
      "         36       0.00      0.00      0.00        13\n",
      "         37       0.00      0.00      0.00        10\n",
      "         38       0.00      0.00      0.00         9\n",
      "         39       0.00      0.00      0.00        12\n",
      "          4       0.00      0.00      0.00         5\n",
      "         40       1.00      0.17      0.29         6\n",
      "         41       0.00      0.00      0.00         7\n",
      "         42       1.00      0.25      0.40         4\n",
      "         43       0.00      0.00      0.00         5\n",
      "         44       0.00      0.00      0.00         7\n",
      "         45       0.00      0.00      0.00         4\n",
      "         46       0.00      0.00      0.00         7\n",
      "         47       0.00      0.00      0.00         8\n",
      "         48       1.00      0.12      0.22         8\n",
      "         49       0.00      0.00      0.00        12\n",
      "          5       0.00      0.00      0.00         8\n",
      "         50       0.00      0.00      0.00         5\n",
      "         51       0.00      0.00      0.00         4\n",
      "         52       0.00      0.00      0.00        15\n",
      "         53       0.00      0.00      0.00         6\n",
      "         54       0.00      0.00      0.00        11\n",
      "         55       0.00      0.00      0.00         3\n",
      "         56       0.00      0.00      0.00         9\n",
      "         57       0.00      0.00      0.00        12\n",
      "         58       0.00      0.00      0.00        10\n",
      "          6       0.00      0.00      0.00        14\n",
      "         60       0.00      0.00      0.00         4\n",
      "         61       0.00      0.00      0.00         5\n",
      "         62       0.00      0.00      0.00         8\n",
      "         63       0.00      0.00      0.00         8\n",
      "         64       0.00      0.00      0.00         5\n",
      "         65       0.00      0.00      0.00         6\n",
      "         66       1.00      0.17      0.29         6\n",
      "         67       0.00      0.00      0.00         9\n",
      "         68       0.01      1.00      0.02         9\n",
      "         69       0.00      0.00      0.00         6\n",
      "          7       0.00      0.00      0.00         9\n",
      "         70       0.00      0.00      0.00        12\n",
      "         71       0.00      0.00      0.00        12\n",
      "         72       0.00      0.00      0.00         5\n",
      "         73       0.00      0.00      0.00         8\n",
      "         74       0.00      0.00      0.00         4\n",
      "         75       0.00      0.00      0.00        10\n",
      "         76       0.00      0.00      0.00         5\n",
      "         77       0.00      0.00      0.00        13\n",
      "         78       0.00      0.00      0.00         3\n",
      "         79       0.00      0.00      0.00         7\n",
      "          8       0.00      0.00      0.00        13\n",
      "         80       0.00      0.00      0.00        10\n",
      "         81       0.00      0.00      0.00         7\n",
      "         82       0.00      0.00      0.00        17\n",
      "         83       0.00      0.00      0.00         7\n",
      "         84       1.00      0.10      0.18        10\n",
      "         85       0.00      0.00      0.00         7\n",
      "         86       0.00      0.00      0.00         6\n",
      "         87       0.00      0.00      0.00         9\n",
      "         88       0.00      0.00      0.00        17\n",
      "         89       0.00      0.00      0.00         7\n",
      "          9       0.00      0.00      0.00        10\n",
      "         90       0.00      0.00      0.00        10\n",
      "         91       0.00      0.00      0.00         9\n",
      "         92       0.00      0.00      0.00         8\n",
      "         93       0.00      0.00      0.00         7\n",
      "         94       0.00      0.00      0.00        13\n",
      "         95       0.00      0.00      0.00         9\n",
      "         96       0.00      0.00      0.00         8\n",
      "         97       0.00      0.00      0.00         9\n",
      "         98       0.00      0.00      0.00         5\n",
      "         99       0.00      0.00      0.00         7\n",
      "\n",
      "avg / total       0.07      0.02      0.02      1000\n",
      "\n",
      "0.9997775119617226 matrics f1 score train data set\n",
      "0.01576412206094559 matrics f1 score test data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(cohen_kappa_score(y_train, pred_Train),'Train data set cohen kappa score')\n",
    "print(cohen_kappa_score(y_test, pred_Test),'Test data set cohen kappa score')\n",
    "\n",
    "#confusion_matrix(y_train, pred_Train)\n",
    "\n",
    "print(accuracy_score(y_train, pred_Train),'Accuracy Score Train Data set')\n",
    "print(accuracy_score(y_test, pred_Test), 'Accuracy Score Test Data set')\n",
    "\n",
    "print(classification_report(y_train, pred_Train))\n",
    "print(classification_report(y_test, pred_Test))\n",
    "\n",
    "print(metrics.f1_score(y_train, pred_Train,average='weighted'),'matrics f1 score train data set')\n",
    "print(metrics.f1_score(y_test, pred_Test,average='weighted'),'matrics f1 score test data set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Copy of sample_EDA.ipynb",
   "provenance": [
    {
     "file_id": "1cFzLXTPFFpojPWQCDFN9WjfVwkzIoI6B",
     "timestamp": 1525119557988
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
