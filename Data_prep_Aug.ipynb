{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Data preparation will include the augmented images. \n",
    "### Steps\n",
    "1. Set the paths to data\n",
    "2. Convert All images to the same dimension  \n",
    "3. Convert to NumPy array\n",
    "4. Change the data type to Unsigned integer (space and file size consideration).\n",
    "5. Create the labels for the images\n",
    "6. Save the data file for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import PIL\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import deepdish as dd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "\n",
    "%matplotlib inline\n",
    "K.set_image_dim_ordering( 'tf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# directory structure of the data\n",
    "\n",
    "dpath_train = '/home/ubuntu/train'      # Data path to training data set\n",
    "dpath_test  = '/home/ubuntu/test'       # Data path to test data set\n",
    "dpath       = '/home/ubuntu'\n",
    "label_f     = '/home/ubuntu/labels.csv' # Path to the label file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start processing the train images\n",
    "* ** Get the data **\n",
    "* ** Resize and convert to numpy array **\n",
    "* ** Add the 4th axis**\n",
    "* ** concatenate images** \n",
    "* ** Store as unsigned** this will keep the file size small and manageable.\n",
    "* ** create the labels for the images** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to location of the Train images\n",
    "images = glob.glob('/home/ubuntu/train/*.jpg')\n",
    "img_data = []\n",
    "# resize and add new axis to images\n",
    "for i in tqdm(images):\n",
    "    img = image.load_img(i, target_size=(224, 224))\n",
    "    img_data.append(image.img_to_array(img)[np.newaxis, :, :, :])\n",
    "# concatenate the images    \n",
    "tr_img_data_np = np.concatenate(img_data, )\n",
    "# save them as \"unsigned Integer\" for the smaller size.\n",
    "tr_img_data_np = tr_img_data_np.astype('uint8')\n",
    "# create the label \n",
    "labels = pd.read_csv(label_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_img_data_np.shape,tr_img_data_np.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point to location of the Test images\n",
    "images_Tst = glob.glob('/home/ubuntu/test/*.jpg')\n",
    "img_data_Tst = []\n",
    "# resize and add new axis to images\n",
    "for i in tqdm(images_Tst):\n",
    "    img = image.load_img(i, target_size=(224, 224))\n",
    "    img_data_Tst.append(image.img_to_array(img)[np.newaxis, :, :, :])\n",
    "# concatenate the images    \n",
    "ts_img_data_np = np.concatenate(img_data_Tst, )\n",
    "# save them as \"unsigned Integer\" for the smaller size.\n",
    "ts_img_data_np = ts_img_data_np.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation \n",
    "\n",
    "* **This function peforms various data augmentation techniques to the dataset**\n",
    "* [Source](https://www.kaggle.com/dhayalkarsahilr/easy-image-augmentation-techniques \"Code Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    @parameters:\n",
    "        dataset: the feature training dataset in numpy array with shape [num_examples, num_rows, num_cols, num_channels] (since it is an image in numpy array)\n",
    "        dataset_labels: the corresponding training labels of the feature training dataset in the same order, and numpy array with shape [num_examples, <anything>]\n",
    "        augmentation_factor: how many times to perform augmentation.\n",
    "        use_random_rotation: whether to use random rotation. default: true\n",
    "        use_random_shift: whether to use random shift. default: true\n",
    "        use_random_shear: whether to use random shear. default: true\n",
    "        use_random_zoom: whether to use random zoom. default: true\n",
    "        \n",
    "    @returns:\n",
    "        augmented_image: augmented dataset\n",
    "        augmented_image_labels: labels corresponding to augmented dataset in order.\n",
    "        \n",
    "    for the augmentation techniques documentation, go here:\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_rotation\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_shear\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_shift\n",
    "        https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing/image/random_zoom\n",
    "'''\n",
    "def augment_data(dataset, dataset_labels, augementation_factor=1, use_random_rotation=True, use_random_shear=True, use_random_shift=True, use_random_zoom=True):\n",
    "    augmented_image = []\n",
    "    augmented_image_labels = []\n",
    "\n",
    "    for num in tqdm(range(0, dataset.shape[0])):  # tqdm progress bar\n",
    "        for i in range(0, augementation_factor):\n",
    "            # original image:\n",
    "            augmented_image.append(dataset[num])\n",
    "            augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_rotation:\n",
    "                \n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_rotation(dataset[num], 20, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shear:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shear(dataset[num], 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_shift:\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_shift(dataset[num], 0.2, 0.2, row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "            if use_random_zoom:\n",
    "                # update: zoomrange (second arg) should be tuple of floats\n",
    "                augmented_image.append(tf.contrib.keras.preprocessing.image.random_zoom(dataset[num], (0.9, 0.9), row_axis=0, col_axis=1, channel_axis=2))\n",
    "                augmented_image_labels.append(dataset_labels[num])\n",
    "\n",
    "    return np.array(augmented_image), np.array(augmented_image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Augmented Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment train data set this will increase the data set size by 5X\n",
    "aug_images, aug_labels = augment_data(tr_img_data_np, labels.breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check point make sure the size and shape are as expected.\n",
    "print(aug_images.shape,aug_labels.shape,aug_images.dtype)\n",
    "# Make sure the data type is expected.\n",
    "aug_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check point make sure the size and shape are as expected.\n",
    "print(ts_img_data_np.shape,ts_img_data_np.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make sure we have all the unique dog names identified and chk the total number.\n",
    "\n",
    "unique_Dog_Breed = []\n",
    "for i in aug_labels:\n",
    "    if i not in unique_Dog_Breed:\n",
    "        unique_Dog_Breed.append(i)\n",
    "    \n",
    "unique_Dog_Breed.sort()   # Now sort the unique lable alphabatically. Next we need to count howmany unique label\n",
    "\n",
    "count = 0\n",
    "unique_Dog_Breed_Num = []   # integer assiciated with every unique Dog name\n",
    "\n",
    "# with the next loops we are creating integers for each unique labels. in prepration of catagorizing the Y.\n",
    "\n",
    "for i in unique_Dog_Breed:\n",
    "    unique_Dog_Breed_Num.append([i, count])\n",
    "    count += 1\n",
    "\n",
    "for i in range(len(aug_labels)):\n",
    "    for j in unique_Dog_Breed_Num:\n",
    "        if aug_labels[i] == j[0]:\n",
    "            aug_labels[i] = j[1]\n",
    "            break\n",
    "\n",
    "print(\"unique Dog Breed Numbers = \",len(unique_Dog_Breed_Num))\n",
    "y_train = to_categorical(aug_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape, y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save test and train data for later usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the X_train and y_train to a HD5 file with Deep Dish. \n",
    "Aug_train_data = '/home/ubuntu/train_aug_data.hdf5'\n",
    "dd.io.save(Aug_train_data, {'X': aug_images, 'Y': aug_labels}, compression=('blosc', 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the test_data as hd5 with deepdish.io\n",
    "test_data = '/home/ubuntu/test_data.hdf5'\n",
    "dd.io.save(test_data, {'X': ts_img_data_np}, compression=('blosc', 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing of the data\n",
    "* Create the X_train and y_train\n",
    "* write the arrays to a file and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to free up some memory\n",
    "del img_data\n",
    "del img_data_np\n",
    "del tr_img_data_np\n",
    "del ts_img_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xs_train = aug_images.astype('float32')/255 \n",
    "Xs_train = aug_images.astype('uint8') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long way to create the X_test data set. for legacy reason is here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the input files Following loop will read the csv file for labels\n",
    "# os.chdir(dpath)\n",
    "# id_label = pd.read_csv(label_f)\n",
    "\n",
    "os.chdir(dpath_test)  # need to be in test data dir\n",
    "# these are the files in the train directory\n",
    "test_dogs = sorted(glob.glob('*.jpg'))\n",
    "im_W = 224\n",
    "im_H = 224\n",
    "#create X_test and y_test  with rescaling the images to IM_W,im_H \n",
    "# rescale is the function we will call in for loop\n",
    "def rescale(image):\n",
    "    org_image = PIL.Image.open(image)\n",
    "    new_image = org_image.resize((im_W, im_H))\n",
    "    tstAry = np.array(new_image)\n",
    "    tstAry = np.expand_dims(tstAry, axis=0)\n",
    "    return tstAry\n",
    "\n",
    "\n",
    "# declearing some varialbles for loop counter and book keeping\n",
    "flag = 0\n",
    "X_test = []  # Test vector\n",
    "\n",
    "Count = 0\n",
    "for w in test_dogs:\n",
    "    Count += 1\n",
    "#    if Count == 1000:  # debug just to see if it is working.\n",
    "#        break\n",
    "    if flag == 0:  # . this is used to handel the first picture. should come up with elegant situation.\n",
    "        X_test = rescale(w)\n",
    "        flag = 1\n",
    "    else:\n",
    "        X_test = np.append(X_test, rescale(w), axis=0)  # creat a numpy array for test.  \n",
    "\n",
    "X_test= X_test.astype( 'uint8' )\n",
    "#X_test= X_test.astype( 'float32' )\n",
    "#X_test = X_test/255   #normalize \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### legacy:  saving files with h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dpath)\n",
    "os.getcwd()\n",
    "# now save to hdf5 (since it's huge I'd like to use hdf5)\n",
    "# also hdf5 is a hierarchical data which lets you assign tags/groups or subgroups to your data\n",
    "import h5py\n",
    "with h5py.File('testdata.hdf5','w') as f:\n",
    "    f.create_dataset('X', data=X_test)\n",
    "\n",
    "print(X_test.shape,Xs_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(dpath)\n",
    "with h5py.File('Aug_traindata.hdf5','w') as f:\n",
    "    f.create_dataset('X', data=aug_images)\n",
    "    f.create_dataset('Y', data=aug_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A shuffling of data just in case we need prior to running the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now shuffling training data. this part was done for the CNN style deeplearning. \n",
    "indx = np.array(list(range(len(aug_images))))\n",
    "np.random.shuffle(indx)\n",
    "print(indx[:10]) #check the shuffling\n",
    "#shuffle data\n",
    "Xs_train = aug_images[indx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
